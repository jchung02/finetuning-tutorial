{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 128000\n",
      "Model max length: 131072\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: Hello, how are you?\n",
      "Token IDs: [128000, 9906, 11, 1268, 527, 499, 30]\n",
      "Token 수: 7\n",
      "\n",
      "  128000 -> '<|begin_of_text|>'\n",
      "    9906 -> 'Hello'\n",
      "      11 -> ','\n",
      "    1268 -> ' how'\n",
      "     527 -> ' are'\n",
      "     499 -> ' you'\n",
      "      30 -> '?'\n",
      "\n",
      "Decoded: <|begin_of_text|>Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "# encode: 문자열 -> token IDs\n",
    "text = \"Hello, how are you?\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"원문: {text}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Token 수: {len(token_ids)}\")\n",
    "print()\n",
    "\n",
    "# 각 token ID가 어떤 문자열에 대응하는지 확인\n",
    "for tid in token_ids:\n",
    "    print(f\"  {tid:>6d} -> '{tokenizer.decode([tid])}'\")\n",
    "\n",
    "print()\n",
    "\n",
    "# decode: token IDs -> 문자열\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== padding='max_length', max_length=15 ===\n",
      "input_ids:      [[128000, 9906, 11, 1268, 527, 499, 30, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "  -> PAD token ID: 128001\n",
      "  -> attention_mask에서 0인 위치가 padding\n"
     ]
    }
   ],
   "source": [
    "# padding: 배치 처리를 위해 길이를 맞춤\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Llama는 pad token이 없으므로 설정 필요\n",
    "\n",
    "# padding=\"max_length\": 지정한 길이까지 PAD 토큰 추가\n",
    "result = tokenizer(\n",
    "    text,\n",
    "    padding=\"max_length\",\n",
    "    max_length=15,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(\"=== padding='max_length', max_length=15 ===\")\n",
    "print(f\"input_ids:      {result['input_ids'].tolist()}\")\n",
    "print(f\"attention_mask: {result['attention_mask'].tolist()}\")\n",
    "print(f\"  -> PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"  -> attention_mask에서 0인 위치가 padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== truncation=True, max_length=10 ===\n",
      "원문 token 수: 17\n",
      "Truncated input_ids: [[128000, 2028, 374, 264, 1633, 1317, 11914, 430, 584, 1390]]\n",
      "Truncated 후 token 수: 10\n",
      "Decoded: '<|begin_of_text|>This is a very long sentence that we want'\n"
     ]
    }
   ],
   "source": [
    "# truncation: 최대 길이 초과 시 잘라냄\n",
    "long_text = \"This is a very long sentence that we want to truncate to a shorter length.\"\n",
    "result_trunc = tokenizer(\n",
    "    long_text,\n",
    "    truncation=True,\n",
    "    max_length=10,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(\"=== truncation=True, max_length=10 ===\")\n",
    "print(f\"원문 token 수: {len(tokenizer.encode(long_text))}\")\n",
    "print(f\"Truncated input_ids: {result_trunc['input_ids'].tolist()}\")\n",
    "print(f\"Truncated 후 token 수: {result_trunc['input_ids'].shape[1]}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(result_trunc['input_ids'][0])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 padding_side: right\n",
      "\n",
      "Right padding: [[128000, 13347, 1070, 128001, 128001, 128001, 128001, 128001, 128001, 128001]]\n",
      "  -> [tokens... PAD PAD PAD]\n",
      "\n",
      "Left padding:  [[128001, 128001, 128001, 128001, 128001, 128001, 128001, 128000, 13347, 1070]]\n",
      "  -> [PAD PAD PAD tokens...]\n"
     ]
    }
   ],
   "source": [
    "# padding side 설정\n",
    "print(f\"현재 padding_side: {tokenizer.padding_side}\")\n",
    "\n",
    "# right padding (default)\n",
    "tokenizer.padding_side = \"right\"\n",
    "right = tokenizer(\"Hi there\", padding=\"max_length\", max_length=10, return_tensors=\"pt\")\n",
    "print(f\"\\nRight padding: {right['input_ids'].tolist()}\")\n",
    "print(f\"  -> [tokens... PAD PAD PAD]\")\n",
    "\n",
    "# left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "left = tokenizer(\"Hi there\", padding=\"max_length\", max_length=10, return_tensors=\"pt\")\n",
    "print(f\"\\nLeft padding:  {left['input_ids'].tolist()}\")\n",
    "print(f\"  -> [PAD PAD PAD tokens...]\")\n",
    "\n",
    "# 다시 기본값으로 복원\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 크기: 52002개\n",
      "\n",
      "--- 예시 1: input이 없는 경우 ---\n",
      "{\n",
      "  \"instruction\": \"Give three tips for staying healthy.\",\n",
      "  \"input\": \"\",\n",
      "  \"output\": \"1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\"\n",
      "}\n",
      "\n",
      "--- 예시 2: input이 있는 경우 (20679개 존재) ---\n",
      "{\n",
      "  \"instruction\": \"Identify the odd one out.\",\n",
      "  \"input\": \"Twitter, Instagram, Telegram\",\n",
      "  \"output\": \"The odd one out is Telegram. Twitter and Instagram are social media platforms mainly for sharing information, images and videos while Telegram is a cloud-based instant messaging and voice-over-IP service.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(\"/teamspace/studios/this_studio/finetuning-tutorial/alpaca_gpt4_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)\n",
    "\n",
    "print(f\"데이터셋 크기: {len(alpaca)}개\")\n",
    "print(f\"\\n--- 예시 1: input이 없는 경우 ---\")\n",
    "print(json.dumps(alpaca[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "# input이 있는 샘플 찾기\n",
    "with_input = [row for row in alpaca if row[\"input\"] != \"\"]\n",
    "print(f\"\\n--- 예시 2: input이 있는 경우 ({len(with_input)}개 존재) ---\")\n",
    "print(json.dumps(with_input[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894f534d38a54c6087f0895eab5d0336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edec78bc32c498cbb3d6d983de163ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d45e89859b477189d712583f4cdfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e44812e87646fca6bae5d51efdeb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat Template (Jinja2) ===\n",
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# Instruct 모델의 tokenizer를 로드해야 chat_template이 있음\n",
    "tokenizer_instruct = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# chat_template 확인\n",
    "print(\"=== Chat Template (Jinja2) ===\")\n",
    "print(tokenizer_instruct.chat_template[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Input: ''\n",
      "Output: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and...\n",
      "\n",
      "=== apply_chat_template 결과 ===\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Give three tips for staying healthy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Alpaca 데이터를 chat format으로 변환\n",
    "row = alpaca[0]  # input이 없는 예시\n",
    "print(f\"Instruction: {row['instruction']}\")\n",
    "print(f\"Input: '{row['input']}'\")\n",
    "print(f\"Output: {row['output'][:100]}...\")\n",
    "print()\n",
    "\n",
    "# chat message 형식으로 구성\n",
    "user_content = row[\"instruction\"]\n",
    "if row[\"input\"]:\n",
    "    user_content += f\"\\n\\n{row['input']}\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": user_content},\n",
    "    {\"role\": \"assistant\", \"content\": row[\"output\"]},\n",
    "]\n",
    "\n",
    "# apply_chat_template으로 변환\n",
    "formatted = tokenizer_instruct.apply_chat_template(messages, tokenize=False)\n",
    "print(\"=== apply_chat_template 결과 ===\")\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Identify the odd one out.\n",
      "Input: Twitter, Instagram, Telegram...\n",
      "\n",
      "=== input이 있는 경우 ===\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Identify the odd one out.\n",
      "\n",
      "Twitter, Instagram, Telegram<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The odd one out is Telegram. Twitter and Instagram are social media platforms mainly for sharing information, images and videos while Telegram is a cloud-based instant messaging and voice-over-IP service.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# input이 있는 샘플도 확인\n",
    "row_with_input = with_input[0]\n",
    "print(f\"Instruction: {row_with_input['instruction']}\")\n",
    "print(f\"Input: {row_with_input['input'][:100]}...\")\n",
    "print()\n",
    "\n",
    "user_content = row_with_input[\"instruction\"] + f\"\\n\\n{row_with_input['input']}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": user_content},\n",
    "    {\"role\": \"assistant\", \"content\": row_with_input[\"output\"]},\n",
    "]\n",
    "\n",
    "formatted = tokenizer_instruct.apply_chat_template(messages, tokenize=False)\n",
    "print(\"=== input이 있는 경우 ===\")\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 수: 2\n",
      "앞부분 token IDs: [Encoding(num_tokens=91, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "\n",
      "=== 앞부분 decode (특수 토큰 포함) ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to tokenizers.Encoding.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== 앞부분 decode (특수 토큰 포함) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(token_ids[:\u001b[38;5;241m15\u001b[39m]):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtid\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m>8d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_instruct\u001b[38;5;241m.\u001b[39mdecode([tid])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to tokenizers.Encoding.__format__"
     ]
    }
   ],
   "source": [
    "# tokenize=True로 하면 바로 token ID로 변환\n",
    "token_ids = tokenizer_instruct.apply_chat_template(messages, tokenize=True)\n",
    "print(f\"Token 수: {len(token_ids)}\")\n",
    "print(f\"앞부분 token IDs: {token_ids[:20]}\")\n",
    "print()\n",
    "\n",
    "# 앞부분 decode해서 special token 확인\n",
    "print(\"=== 앞부분 decode (특수 토큰 포함) ===\")\n",
    "for i, tid in enumerate(token_ids[:15]):\n",
    "    print(f\"  [{i:2d}] {tid:>8d} -> '{tokenizer_instruct.decode([tid])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== special_tokens_map ===\n",
      "  bos_token: <|begin_of_text|>\n",
      "  eos_token: <|end_of_text|>\n",
      "  pad_token: <|end_of_text|>\n",
      "\n",
      "=== 개별 확인 ===\n",
      "BOS token: '<|begin_of_text|>' (ID: 128000)\n",
      "EOS token: '<|end_of_text|>' (ID: 128001)\n",
      "PAD token: '<|end_of_text|>' (ID: 128001)\n",
      "UNK token: 'None' (ID: None)\n"
     ]
    }
   ],
   "source": [
    "# special_tokens_map으로 전체 확인\n",
    "print(\"=== special_tokens_map ===\")\n",
    "for key, value in tokenizer.special_tokens_map.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n=== 개별 확인 ===\")\n",
    "print(f\"BOS token: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"UNK token: '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Llama 3 추가 special tokens ===\n",
      "  [128000] <|begin_of_text|>\n",
      "  [128001] <|end_of_text|>\n",
      "  [128002] <|reserved_special_token_0|>\n",
      "  [128003] <|reserved_special_token_1|>\n",
      "  [128004] <|finetune_right_pad_id|>\n",
      "  [128005] <|reserved_special_token_2|>\n",
      "  [128006] <|start_header_id|>\n",
      "  [128007] <|end_header_id|>\n",
      "  [128008] <|eom_id|>\n",
      "  [128009] <|eot_id|>\n",
      "  [128010] <|python_tag|>\n",
      "  [128011] <|reserved_special_token_3|>\n",
      "  [128012] <|reserved_special_token_4|>\n",
      "  [128013] <|reserved_special_token_5|>\n",
      "  [128014] <|reserved_special_token_6|>\n",
      "  [128015] <|reserved_special_token_7|>\n",
      "  [128016] <|reserved_special_token_8|>\n",
      "  [128017] <|reserved_special_token_9|>\n",
      "  [128018] <|reserved_special_token_10|>\n",
      "  [128019] <|reserved_special_token_11|>\n",
      "  [128020] <|reserved_special_token_12|>\n",
      "  [128021] <|reserved_special_token_13|>\n",
      "  [128022] <|reserved_special_token_14|>\n",
      "  [128023] <|reserved_special_token_15|>\n",
      "  [128024] <|reserved_special_token_16|>\n",
      "  [128025] <|reserved_special_token_17|>\n",
      "  [128026] <|reserved_special_token_18|>\n",
      "  [128027] <|reserved_special_token_19|>\n",
      "  [128028] <|reserved_special_token_20|>\n",
      "  [128029] <|reserved_special_token_21|>\n",
      "  [128030] <|reserved_special_token_22|>\n",
      "  [128031] <|reserved_special_token_23|>\n",
      "  [128032] <|reserved_special_token_24|>\n",
      "  [128033] <|reserved_special_token_25|>\n",
      "  [128034] <|reserved_special_token_26|>\n",
      "  [128035] <|reserved_special_token_27|>\n",
      "  [128036] <|reserved_special_token_28|>\n",
      "  [128037] <|reserved_special_token_29|>\n",
      "  [128038] <|reserved_special_token_30|>\n",
      "  [128039] <|reserved_special_token_31|>\n",
      "  [128040] <|reserved_special_token_32|>\n",
      "  [128041] <|reserved_special_token_33|>\n",
      "  [128042] <|reserved_special_token_34|>\n",
      "  [128043] <|reserved_special_token_35|>\n",
      "  [128044] <|reserved_special_token_36|>\n",
      "  [128045] <|reserved_special_token_37|>\n",
      "  [128046] <|reserved_special_token_38|>\n",
      "  [128047] <|reserved_special_token_39|>\n",
      "  [128048] <|reserved_special_token_40|>\n",
      "  [128049] <|reserved_special_token_41|>\n",
      "  [128050] <|reserved_special_token_42|>\n",
      "  [128051] <|reserved_special_token_43|>\n",
      "  [128052] <|reserved_special_token_44|>\n",
      "  [128053] <|reserved_special_token_45|>\n",
      "  [128054] <|reserved_special_token_46|>\n",
      "  [128055] <|reserved_special_token_47|>\n",
      "  [128056] <|reserved_special_token_48|>\n",
      "  [128057] <|reserved_special_token_49|>\n",
      "  [128058] <|reserved_special_token_50|>\n",
      "  [128059] <|reserved_special_token_51|>\n",
      "  [128060] <|reserved_special_token_52|>\n",
      "  [128061] <|reserved_special_token_53|>\n",
      "  [128062] <|reserved_special_token_54|>\n",
      "  [128063] <|reserved_special_token_55|>\n",
      "  [128064] <|reserved_special_token_56|>\n",
      "  [128065] <|reserved_special_token_57|>\n",
      "  [128066] <|reserved_special_token_58|>\n",
      "  [128067] <|reserved_special_token_59|>\n",
      "  [128068] <|reserved_special_token_60|>\n",
      "  [128069] <|reserved_special_token_61|>\n",
      "  [128070] <|reserved_special_token_62|>\n",
      "  [128071] <|reserved_special_token_63|>\n",
      "  [128072] <|reserved_special_token_64|>\n",
      "  [128073] <|reserved_special_token_65|>\n",
      "  [128074] <|reserved_special_token_66|>\n",
      "  [128075] <|reserved_special_token_67|>\n",
      "  [128076] <|reserved_special_token_68|>\n",
      "  [128077] <|reserved_special_token_69|>\n",
      "  [128078] <|reserved_special_token_70|>\n",
      "  [128079] <|reserved_special_token_71|>\n",
      "  [128080] <|reserved_special_token_72|>\n",
      "  [128081] <|reserved_special_token_73|>\n",
      "  [128082] <|reserved_special_token_74|>\n",
      "  [128083] <|reserved_special_token_75|>\n",
      "  [128084] <|reserved_special_token_76|>\n",
      "  [128085] <|reserved_special_token_77|>\n",
      "  [128086] <|reserved_special_token_78|>\n",
      "  [128087] <|reserved_special_token_79|>\n",
      "  [128088] <|reserved_special_token_80|>\n",
      "  [128089] <|reserved_special_token_81|>\n",
      "  [128090] <|reserved_special_token_82|>\n",
      "  [128091] <|reserved_special_token_83|>\n",
      "  [128092] <|reserved_special_token_84|>\n",
      "  [128093] <|reserved_special_token_85|>\n",
      "  [128094] <|reserved_special_token_86|>\n",
      "  [128095] <|reserved_special_token_87|>\n",
      "  [128096] <|reserved_special_token_88|>\n",
      "  [128097] <|reserved_special_token_89|>\n",
      "  [128098] <|reserved_special_token_90|>\n",
      "  [128099] <|reserved_special_token_91|>\n",
      "  [128100] <|reserved_special_token_92|>\n",
      "  [128101] <|reserved_special_token_93|>\n",
      "  [128102] <|reserved_special_token_94|>\n",
      "  [128103] <|reserved_special_token_95|>\n",
      "  [128104] <|reserved_special_token_96|>\n",
      "  [128105] <|reserved_special_token_97|>\n",
      "  [128106] <|reserved_special_token_98|>\n",
      "  [128107] <|reserved_special_token_99|>\n",
      "  [128108] <|reserved_special_token_100|>\n",
      "  [128109] <|reserved_special_token_101|>\n",
      "  [128110] <|reserved_special_token_102|>\n",
      "  [128111] <|reserved_special_token_103|>\n",
      "  [128112] <|reserved_special_token_104|>\n",
      "  [128113] <|reserved_special_token_105|>\n",
      "  [128114] <|reserved_special_token_106|>\n",
      "  [128115] <|reserved_special_token_107|>\n",
      "  [128116] <|reserved_special_token_108|>\n",
      "  [128117] <|reserved_special_token_109|>\n",
      "  [128118] <|reserved_special_token_110|>\n",
      "  [128119] <|reserved_special_token_111|>\n",
      "  [128120] <|reserved_special_token_112|>\n",
      "  [128121] <|reserved_special_token_113|>\n",
      "  [128122] <|reserved_special_token_114|>\n",
      "  [128123] <|reserved_special_token_115|>\n",
      "  [128124] <|reserved_special_token_116|>\n",
      "  [128125] <|reserved_special_token_117|>\n",
      "  [128126] <|reserved_special_token_118|>\n",
      "  [128127] <|reserved_special_token_119|>\n",
      "  [128128] <|reserved_special_token_120|>\n",
      "  [128129] <|reserved_special_token_121|>\n",
      "  [128130] <|reserved_special_token_122|>\n",
      "  [128131] <|reserved_special_token_123|>\n",
      "  [128132] <|reserved_special_token_124|>\n",
      "  [128133] <|reserved_special_token_125|>\n",
      "  [128134] <|reserved_special_token_126|>\n",
      "  [128135] <|reserved_special_token_127|>\n",
      "  [128136] <|reserved_special_token_128|>\n",
      "  [128137] <|reserved_special_token_129|>\n",
      "  [128138] <|reserved_special_token_130|>\n",
      "  [128139] <|reserved_special_token_131|>\n",
      "  [128140] <|reserved_special_token_132|>\n",
      "  [128141] <|reserved_special_token_133|>\n",
      "  [128142] <|reserved_special_token_134|>\n",
      "  [128143] <|reserved_special_token_135|>\n",
      "  [128144] <|reserved_special_token_136|>\n",
      "  [128145] <|reserved_special_token_137|>\n",
      "  [128146] <|reserved_special_token_138|>\n",
      "  [128147] <|reserved_special_token_139|>\n",
      "  [128148] <|reserved_special_token_140|>\n",
      "  [128149] <|reserved_special_token_141|>\n",
      "  [128150] <|reserved_special_token_142|>\n",
      "  [128151] <|reserved_special_token_143|>\n",
      "  [128152] <|reserved_special_token_144|>\n",
      "  [128153] <|reserved_special_token_145|>\n",
      "  [128154] <|reserved_special_token_146|>\n",
      "  [128155] <|reserved_special_token_147|>\n",
      "  [128156] <|reserved_special_token_148|>\n",
      "  [128157] <|reserved_special_token_149|>\n",
      "  [128158] <|reserved_special_token_150|>\n",
      "  [128159] <|reserved_special_token_151|>\n",
      "  [128160] <|reserved_special_token_152|>\n",
      "  [128161] <|reserved_special_token_153|>\n",
      "  [128162] <|reserved_special_token_154|>\n",
      "  [128163] <|reserved_special_token_155|>\n",
      "  [128164] <|reserved_special_token_156|>\n",
      "  [128165] <|reserved_special_token_157|>\n",
      "  [128166] <|reserved_special_token_158|>\n",
      "  [128167] <|reserved_special_token_159|>\n",
      "  [128168] <|reserved_special_token_160|>\n",
      "  [128169] <|reserved_special_token_161|>\n",
      "  [128170] <|reserved_special_token_162|>\n",
      "  [128171] <|reserved_special_token_163|>\n",
      "  [128172] <|reserved_special_token_164|>\n",
      "  [128173] <|reserved_special_token_165|>\n",
      "  [128174] <|reserved_special_token_166|>\n",
      "  [128175] <|reserved_special_token_167|>\n",
      "  [128176] <|reserved_special_token_168|>\n",
      "  [128177] <|reserved_special_token_169|>\n",
      "  [128178] <|reserved_special_token_170|>\n",
      "  [128179] <|reserved_special_token_171|>\n",
      "  [128180] <|reserved_special_token_172|>\n",
      "  [128181] <|reserved_special_token_173|>\n",
      "  [128182] <|reserved_special_token_174|>\n",
      "  [128183] <|reserved_special_token_175|>\n",
      "  [128184] <|reserved_special_token_176|>\n",
      "  [128185] <|reserved_special_token_177|>\n",
      "  [128186] <|reserved_special_token_178|>\n",
      "  [128187] <|reserved_special_token_179|>\n",
      "  [128188] <|reserved_special_token_180|>\n",
      "  [128189] <|reserved_special_token_181|>\n",
      "  [128190] <|reserved_special_token_182|>\n",
      "  [128191] <|reserved_special_token_183|>\n",
      "  [128192] <|reserved_special_token_184|>\n",
      "  [128193] <|reserved_special_token_185|>\n",
      "  [128194] <|reserved_special_token_186|>\n",
      "  [128195] <|reserved_special_token_187|>\n",
      "  [128196] <|reserved_special_token_188|>\n",
      "  [128197] <|reserved_special_token_189|>\n",
      "  [128198] <|reserved_special_token_190|>\n",
      "  [128199] <|reserved_special_token_191|>\n",
      "  [128200] <|reserved_special_token_192|>\n",
      "  [128201] <|reserved_special_token_193|>\n",
      "  [128202] <|reserved_special_token_194|>\n",
      "  [128203] <|reserved_special_token_195|>\n",
      "  [128204] <|reserved_special_token_196|>\n",
      "  [128205] <|reserved_special_token_197|>\n",
      "  [128206] <|reserved_special_token_198|>\n",
      "  [128207] <|reserved_special_token_199|>\n",
      "  [128208] <|reserved_special_token_200|>\n",
      "  [128209] <|reserved_special_token_201|>\n",
      "  [128210] <|reserved_special_token_202|>\n",
      "  [128211] <|reserved_special_token_203|>\n",
      "  [128212] <|reserved_special_token_204|>\n",
      "  [128213] <|reserved_special_token_205|>\n",
      "  [128214] <|reserved_special_token_206|>\n",
      "  [128215] <|reserved_special_token_207|>\n",
      "  [128216] <|reserved_special_token_208|>\n",
      "  [128217] <|reserved_special_token_209|>\n",
      "  [128218] <|reserved_special_token_210|>\n",
      "  [128219] <|reserved_special_token_211|>\n",
      "  [128220] <|reserved_special_token_212|>\n",
      "  [128221] <|reserved_special_token_213|>\n",
      "  [128222] <|reserved_special_token_214|>\n",
      "  [128223] <|reserved_special_token_215|>\n",
      "  [128224] <|reserved_special_token_216|>\n",
      "  [128225] <|reserved_special_token_217|>\n",
      "  [128226] <|reserved_special_token_218|>\n",
      "  [128227] <|reserved_special_token_219|>\n",
      "  [128228] <|reserved_special_token_220|>\n",
      "  [128229] <|reserved_special_token_221|>\n",
      "  [128230] <|reserved_special_token_222|>\n",
      "  [128231] <|reserved_special_token_223|>\n",
      "  [128232] <|reserved_special_token_224|>\n",
      "  [128233] <|reserved_special_token_225|>\n",
      "  [128234] <|reserved_special_token_226|>\n",
      "  [128235] <|reserved_special_token_227|>\n",
      "  [128236] <|reserved_special_token_228|>\n",
      "  [128237] <|reserved_special_token_229|>\n",
      "  [128238] <|reserved_special_token_230|>\n",
      "  [128239] <|reserved_special_token_231|>\n",
      "  [128240] <|reserved_special_token_232|>\n",
      "  [128241] <|reserved_special_token_233|>\n",
      "  [128242] <|reserved_special_token_234|>\n",
      "  [128243] <|reserved_special_token_235|>\n",
      "  [128244] <|reserved_special_token_236|>\n",
      "  [128245] <|reserved_special_token_237|>\n",
      "  [128246] <|reserved_special_token_238|>\n",
      "  [128247] <|reserved_special_token_239|>\n",
      "  [128248] <|reserved_special_token_240|>\n",
      "  [128249] <|reserved_special_token_241|>\n",
      "  [128250] <|reserved_special_token_242|>\n",
      "  [128251] <|reserved_special_token_243|>\n",
      "  [128252] <|reserved_special_token_244|>\n",
      "  [128253] <|reserved_special_token_245|>\n",
      "  [128254] <|reserved_special_token_246|>\n",
      "  [128255] <|reserved_special_token_247|>\n"
     ]
    }
   ],
   "source": [
    "# Llama 3의 추가 special token 확인\n",
    "print(\"=== Llama 3 추가 special tokens ===\")\n",
    "added_tokens = tokenizer.added_tokens_encoder\n",
    "for token, idx in sorted(added_tokens.items(), key=lambda x: x[1]):\n",
    "    print(f\"  [{idx:>6d}] {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved special tokens: 248개\n",
      "예시: ['<|reserved_special_token_0|>', '<|reserved_special_token_1|>', '<|reserved_special_token_2|>', '<|reserved_special_token_3|>', '<|reserved_special_token_4|>']\n",
      "\n",
      "-> vocab size가 고정이므로, 미리 예약된 자리를 활용할 수 있음\n"
     ]
    }
   ],
   "source": [
    "# Reserved special token 확인\n",
    "# Llama 3는 미리 예약된 빈 슬롯을 가지고 있음\n",
    "reserved = [t for t in added_tokens.keys() if \"reserved_special_token\" in t]\n",
    "print(f\"Reserved special tokens: {len(reserved)}개\")\n",
    "print(f\"예시: {reserved[:5]}\")\n",
    "print(f\"\\n-> vocab size가 고정이므로, 미리 예약된 자리를 활용할 수 있음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd196ea958e6492297bfb9690a8034ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추가 전 vocab size: 128256\n",
      "추가 전 embedding shape: torch.Size([128256, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"추가 전 vocab size: {len(tokenizer)}\")\n",
    "print(f\"추가 전 embedding shape: {model.get_input_embeddings().weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추가된 토큰 수: 3\n",
      "추가 후 vocab size: 128259\n",
      "  '<TIME>' -> ID: 128256\n",
      "  '<DATE>' -> ID: 128257\n",
      "  '<LOCATION>' -> ID: 128258\n"
     ]
    }
   ],
   "source": [
    "# 새로운 special token 추가\n",
    "new_tokens = [\"<TIME>\", \"<DATE>\", \"<LOCATION>\"]\n",
    "\n",
    "num_added = tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": new_tokens\n",
    "})\n",
    "print(f\"추가된 토큰 수: {num_added}\")\n",
    "print(f\"추가 후 vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# 새 토큰 확인\n",
    "for token in new_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"  '{token}' -> ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resize 후 embedding shape: torch.Size([128259, 4096])\n",
      "\n",
      "원문: today we have meeting at <DATE> Seoul <LOCATION>\n",
      "Token IDs: [128000, 31213, 584, 617, 6574, 520, 220, 128257, 51289, 220, 128258]\n",
      "\n",
      "    128000 -> '<|begin_of_text|>'\n",
      "     31213 -> 'today'\n",
      "       584 -> ' we'\n",
      "       617 -> ' have'\n",
      "      6574 -> ' meeting'\n",
      "       520 -> ' at'\n",
      "       220 -> ' '\n",
      "    128257 -> '<DATE>'\n",
      "     51289 -> ' Seoul'\n",
      "       220 -> ' '\n",
      "    128258 -> '<LOCATION>'\n",
      "\n",
      "⚠️ 새 토큰의 embedding은 random init 상태 -> fine-tuning으로 학습 필요!\n"
     ]
    }
   ],
   "source": [
    "# embedding layer resize\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resize 후 embedding shape: {model.get_input_embeddings().weight.shape}\")\n",
    "print()\n",
    "\n",
    "# 새 토큰이 제대로 tokenize되는지 확인\n",
    "test_text = \"today we have meeting at <DATE> Seoul <LOCATION>\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"원문: {test_text}\")\n",
    "print(f\"Token IDs: {encoded}\")\n",
    "print()\n",
    "for tid in encoded:\n",
    "    print(f\"  {tid:>8d} -> '{tokenizer.decode([tid])}'\")\n",
    "\n",
    "print(f\"\\n⚠️ 새 토큰의 embedding은 random init 상태 -> fine-tuning으로 학습 필요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Causal LM: Next Token Prediction ===\n",
      "Input:  ['<|begin_of_text|>', 'The', ' cat', ' sat', ' on', ' the']\n",
      "Label:  ['The', ' cat', ' sat', ' on', ' the', ' mat']\n",
      "\n",
      "  Step 0: '<|begin_of_text|>' -> 예측 대상: 'The'\n",
      "  Step 1: 'The' -> 예측 대상: ' cat'\n",
      "  Step 2: ' cat' -> 예측 대상: ' sat'\n",
      "  Step 3: ' sat' -> 예측 대상: ' on'\n",
      "  Step 4: ' on' -> 예측 대상: ' the'\n",
      "  Step 5: ' the' -> 예측 대상: ' mat'\n"
     ]
    }
   ],
   "source": [
    "# 1) 기본 Causal LM label: input을 한 칸 shift\n",
    "text = \"The cat sat on the mat\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "\n",
    "input_tokens = tokens[:-1]\n",
    "label_tokens = tokens[1:]\n",
    "\n",
    "print(\"=== Causal LM: Next Token Prediction ===\")\n",
    "print(f\"Input:  {input_tokens}\")\n",
    "print(f\"Label:  {label_tokens}\")\n",
    "print()\n",
    "for i, (inp, lbl) in enumerate(zip(input_tokens, label_tokens)):\n",
    "    print(f\"  Step {i}: '{inp}' -> 예측 대상: '{lbl}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SFT Label Masking ===\n",
      "전체 token 수: 15\n",
      "Instruction token 수: 8 (이 부분은 loss에서 제외)\n",
      "\n",
      "               Token |   Token ID |      Label | Loss 계산?\n",
      "-----------------------------------------------------------------\n",
      "   <|begin_of_text|> |     128000 |       -100 | ❌ 제외\n",
      "                What |       3923 |       -100 | ❌ 제외\n",
      "                  is |        374 |       -100 | ❌ 제외\n",
      "                 the |        279 |       -100 | ❌ 제외\n",
      "             capital |       6864 |       -100 | ❌ 제외\n",
      "                  of |        315 |       -100 | ❌ 제외\n",
      "              France |       9822 |       -100 | ❌ 제외\n",
      "                   ? |         30 |       -100 | ❌ 제외\n",
      "                 The |        578 |        578 | ✅ 계산\n",
      "             capital |       6864 |       6864 | ✅ 계산\n",
      "                  of |        315 |        315 | ✅ 계산\n",
      "              France |       9822 |       9822 | ✅ 계산\n",
      "                  is |        374 |        374 | ✅ 계산\n",
      "               Paris |      12366 |      12366 | ✅ 계산\n",
      "                   . |         13 |         13 | ✅ 계산\n"
     ]
    }
   ],
   "source": [
    "# 2) SFT Label Masking 시연\n",
    "# Instruction 부분은 -100으로 masking, response 부분만 loss 계산\n",
    "\n",
    "instruction = \"What is the capital of France?\"\n",
    "response = \"The capital of France is Paris.\"\n",
    "full_text = instruction + \" \" + response\n",
    "\n",
    "# tokenize\n",
    "instruction_ids = tokenizer.encode(instruction)\n",
    "full_ids = tokenizer.encode(full_text)\n",
    "\n",
    "# label 생성: instruction 부분은 -100, response 부분은 실제 token ID\n",
    "labels = [-100] * len(instruction_ids) + full_ids[len(instruction_ids):]\n",
    "\n",
    "print(\"=== SFT Label Masking ===\")\n",
    "print(f\"전체 token 수: {len(full_ids)}\")\n",
    "print(f\"Instruction token 수: {len(instruction_ids)} (이 부분은 loss에서 제외)\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Token':>20s} | {'Token ID':>10s} | {'Label':>10s} | Loss 계산?\")\n",
    "print(\"-\" * 65)\n",
    "for i, (tid, lbl) in enumerate(zip(full_ids, labels)):\n",
    "    token_str = tokenizer.decode([tid])\n",
    "    loss_yn = \"❌ 제외\" if lbl == -100 else \"✅ 계산\"\n",
    "    print(f\"{token_str:>20s} | {tid:>10d} | {lbl:>10d} | {loss_yn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Packing 전: 개별 padding ===\n",
      "  [TTTTTTTT____________] 실제:8 pad:12\n",
      "  [TTTTTT______________] 실제:6 pad:14\n",
      "  [TTTTTTTT____________] 실제:8 pad:12\n",
      "  [TTTTTTT_____________] 실제:7 pad:13\n",
      "\n",
      "  전체 80 tokens 중 padding: 51 (63.7% 낭비!)\n"
     ]
    }
   ],
   "source": [
    "# Packing 전: 개별 샘플 + padding\n",
    "samples = [\n",
    "    \"Give three tips for staying healthy.\",\n",
    "    \"What are primary colors?\",\n",
    "    \"Describe the structure of an atom.\",\n",
    "    \"How does photosynthesis work?\",\n",
    "]\n",
    "\n",
    "max_len = 20  # 시연용 짧은 길이\n",
    "\n",
    "print(\"=== Packing 전: 개별 padding ===\")\n",
    "total_tokens = 0\n",
    "total_pad = 0\n",
    "for s in samples:\n",
    "    ids = tokenizer.encode(s)\n",
    "    pad_count = max_len - len(ids)\n",
    "    total_tokens += max_len\n",
    "    total_pad += pad_count\n",
    "    \n",
    "    # 시각화: T=실제토큰, _=padding\n",
    "    visual = \"\".join([\"T\" for _ in ids] + [\"_\" for _ in range(pad_count)])\n",
    "    print(f\"  [{visual}] 실제:{len(ids)} pad:{pad_count}\")\n",
    "\n",
    "print(f\"\\n  전체 {total_tokens} tokens 중 padding: {total_pad} ({total_pad/total_tokens*100:.1f}% 낭비!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Packing 후: 연결된 시퀀스 ===\n",
      "  Seq 0: [TTTTTTTT|TTTTTT|TTTT]\n",
      "  Seq 1: [TTTT|TTTTTTT||||||||]\n",
      "\n",
      "  전체 40 tokens 중 padding: 7 (17.5% 낭비)\n",
      "  -> 기존 63.7% 낭비에서 17.5%로 개선!\n"
     ]
    }
   ],
   "source": [
    "# Packing 후: 샘플들을 EOS로 연결\n",
    "print(\"=== Packing 후: 연결된 시퀀스 ===\")\n",
    "\n",
    "all_ids = []\n",
    "for s in samples:\n",
    "    ids = tokenizer.encode(s)\n",
    "    all_ids.extend(ids + [tokenizer.eos_token_id])  # EOS로 구분\n",
    "\n",
    "# max_seq_len 단위로 잘라서 packed sequence 생성\n",
    "packed_seqs = []\n",
    "for i in range(0, len(all_ids), max_len):\n",
    "    seq = all_ids[i : i + max_len]\n",
    "    pad_count = max_len - len(seq)\n",
    "    packed_seqs.append(seq + [tokenizer.pad_token_id] * pad_count)\n",
    "\n",
    "total_tokens_packed = len(packed_seqs) * max_len\n",
    "total_real = len(all_ids)\n",
    "total_pad_packed = total_tokens_packed - total_real\n",
    "\n",
    "for i, seq in enumerate(packed_seqs):\n",
    "    visual = \"\"\n",
    "    for tid in seq:\n",
    "        if tid == tokenizer.pad_token_id and tid != tokenizer.eos_token_id:\n",
    "            visual += \"_\"\n",
    "        elif tid == tokenizer.eos_token_id:\n",
    "            visual += \"|\"  # EOS를 구분자로 표시\n",
    "        else:\n",
    "            visual += \"T\"\n",
    "    print(f\"  Seq {i}: [{visual}]\")\n",
    "\n",
    "print(f\"\\n  전체 {total_tokens_packed} tokens 중 padding: {max(total_pad_packed, 0)} ({max(total_pad_packed,0)/total_tokens_packed*100:.1f}% 낭비)\")\n",
    "print(f\"  -> 기존 {total_pad/total_tokens*100:.1f}% 낭비에서 {max(total_pad_packed,0)/total_tokens_packed*100:.1f}%로 개선!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed sequences: 1개\n",
      "\n",
      "  Seq 0:\n",
      "    input_ids (15): [128000, 36227, 2380, 10631, 369, 19994, 9498, 13, 128000, 3923]...\n",
      "    labels    (15): [36227, 2380, 10631, 369, 19994, 9498, 13, 128000, 3923, 527]...\n",
      "    -> input을 1칸 shift한 것이 label!\n"
     ]
    }
   ],
   "source": [
    "# 실제 vanilla_train.py의 pack 함수와 동일한 방식\n",
    "def pack(dataset, tokenizer, max_seq_len=1024):\n",
    "    \"\"\"Packing: 모든 샘플을 이어붙이고 max_seq_len+1 단위로 잘라서\n",
    "    input_ids와 labels (1칸 shift) 생성\"\"\"\n",
    "    all_token_ids = []\n",
    "    for text in dataset:\n",
    "        all_token_ids.extend(tokenizer.encode(text))\n",
    "\n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len + 1):\n",
    "        chunk = all_token_ids[i : i + max_seq_len + 1]\n",
    "        if len(chunk) == (max_seq_len + 1):\n",
    "            packed_ds.append({\n",
    "                \"input_ids\": chunk[:-1],   # [0:max_seq_len]\n",
    "                \"labels\": chunk[1:],        # [1:max_seq_len+1] (1칸 shift)\n",
    "            })\n",
    "    return packed_ds\n",
    "\n",
    "# 샘플 데이터로 실행\n",
    "packed = pack(samples, tokenizer, max_seq_len=15)\n",
    "print(f\"Packed sequences: {len(packed)}개\")\n",
    "for i, p in enumerate(packed):\n",
    "    print(f\"\\n  Seq {i}:\")\n",
    "    print(f\"    input_ids ({len(p['input_ids'])}): {p['input_ids'][:10]}...\")\n",
    "    print(f\"    labels    ({len(p['labels'])}): {p['labels'][:10]}...\")\n",
    "    print(f\"    -> input을 1칸 shift한 것이 label!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
